{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sK2IqYYnxfC6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "#from wordcloud import WordCloud\n",
    "\n",
    "#Input data files are available in the read-only \"../input/\" directory\n",
    "#For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CrgX62V1xfC8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import inflect\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import re, string, unicodedata\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "BXtg8ee4xfC9",
    "outputId": "d4d7bf26-2ad9-4db1-b9cc-ce8609551a85",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment                                            content\n",
       "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...\n",
       "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2  1956967696     sadness                Funeral ceremony...gloomy friday...\n",
       "3  1956967789  enthusiasm               wants to hang out with friends SOON!\n",
       "4  1956968416     neutral  @dannycastillo We want to trade with someone w..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('tweet_emotions.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xedVzuYXxfC-",
    "outputId": "b26821a2-c53c-4678-ace4-65dc0b1908c3",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q_XAnd4mxfC_",
    "outputId": "5cd2a74d-eaa6-47b6-e78a-623df938f60d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C_ncEmw9xfC_",
    "outputId": "97f2be19-8997-429d-dd6a-c5ba3ad50e00",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MVOzUnLixfC_",
    "outputId": "4f9eec99-64ea-4ed1-b800-301e094ad73a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_PjPDWz1xfDA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop('tweet_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5p1BphqZxfDA",
    "outputId": "de284a8f-1afd-4de4-e073-9e4256d6df82",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "id": "FAVVHj1rxfDA",
    "outputId": "d92fd05c-5fa0-4f02-b405-33923cb9f1e9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.countplot(x=df['sentiment'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "id": "AjiaDMDpxfDA",
    "outputId": "fbe5002f-f9d3-4b10-e35c-ea8a1ccb31fb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.histplot(x=df['sentiment'], kde=True)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1prt9nC2xfDB",
    "outputId": "dd027ee9-ed3c-4345-c283-198068f8cb40",
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2tj3rQYxfDB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeULmsmVxfDB"
   },
   "source": [
    "# Remove junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z6XAfnl3xfDC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_preprocessing_platform(df, text_col, remove_stopwords=True):\n",
    "\n",
    "    ## Define functions for individual steps\n",
    "    # First function is used to denoise text\n",
    "    def denoise_text(text):\n",
    "        # Strip html if any. For ex. removing <html>, <p> tags\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        text = soup.get_text()\n",
    "        # Replace contractions in the text. For ex. didn't -> did not\n",
    "        text = contractions.fix(text)\n",
    "        return text\n",
    "\n",
    "    ## Next step is text-normalization\n",
    "\n",
    "    # Text normalization includes many steps.\n",
    "\n",
    "    # Each function below serves a step.\n",
    "\n",
    "\n",
    "    def remove_non_ascii(words):\n",
    "        \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            new_words.append(new_word)\n",
    "        return new_words\n",
    "\n",
    "\n",
    "    def to_lowercase(words):\n",
    "        \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_word = word.lower()\n",
    "            new_words.append(new_word)\n",
    "        return new_words\n",
    "\n",
    "\n",
    "    def remove_punctuation(words):\n",
    "        \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "            if new_word != '':\n",
    "                new_words.append(new_word)\n",
    "        return new_words\n",
    "\n",
    "\n",
    "    def replace_numbers(words):\n",
    "        \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "        p = inflect.engine()\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if word.isdigit():\n",
    "                new_word = p.number_to_words(word)\n",
    "                new_words.append(new_word)\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "        return new_words\n",
    "\n",
    "\n",
    "    def remove_stopwords(words):\n",
    "        \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if word not in stopwords.words('english'):\n",
    "                new_words.append(word)\n",
    "        return new_words\n",
    "\n",
    "\n",
    "    def stem_words(words):\n",
    "        \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "        stemmer = LancasterStemmer()\n",
    "        stems = []\n",
    "        for word in words:\n",
    "            stem = stemmer.stem(word)\n",
    "            stems.append(stem)\n",
    "        return stems\n",
    "\n",
    "\n",
    "    def lemmatize_verbs(words):\n",
    "        \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmas = []\n",
    "        for word in words:\n",
    "            lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "            lemmas.append(lemma)\n",
    "        return lemmas\n",
    "\n",
    "\n",
    "    ### A wrap-up function for normalization\n",
    "    def normalize_text(words, remove_stopwords):\n",
    "        words = remove_non_ascii(words)\n",
    "        words = to_lowercase(words)\n",
    "        words = remove_punctuation(words)\n",
    "        words = replace_numbers(words)\n",
    "        if remove_stopwords:\n",
    "            words = remove_stopwords(words)\n",
    "        #words = stem_words(words)\n",
    "        words = lemmatize_verbs(words)\n",
    "        return words\n",
    "\n",
    "    # All above functions work on word tokens we need a tokenizer\n",
    "\n",
    "    # Tokenize tweet into words\n",
    "    def tokenize(text):\n",
    "        return nltk.word_tokenize(text)\n",
    "\n",
    "\n",
    "    # A overall wrap-up function\n",
    "    def text_prepare(text):\n",
    "        text = denoise_text(text)\n",
    "        text = ' '.join([x for x in normalize_text(tokenize(text), remove_stopwords)])\n",
    "        return text\n",
    "\n",
    "    # run every-step\n",
    "    df[text_col] = [text_prepare(x) for x in df[text_col]]\n",
    "\n",
    "\n",
    "    # return processed df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4ASBT9AxfDD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = text_preprocessing_platform(df, 'content', remove_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "06kjSfQdxfDD",
    "outputId": "e186c2d4-5823-49e7-890d-07ad4b48efea",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mQd5wDUxxfDE",
    "outputId": "7ba02296-3c7c-4210-beec-d3fa99cf07a0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1T-YlaHxfDE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def categorize_sentiment(row):\n",
    "    if row['sentiment'] in ['empty', 'sadness', 'anger', 'worry', 'hate', 'boredom']:\n",
    "        return 'Concerned'\n",
    "    elif row['sentiment'] in ['neutral','relief','surprise','enthusiasm', 'happiness', 'love', 'fun']:  # Note: 'empty' appears again; check if this is correct.\n",
    "        return 'Neutral'\n",
    "    #elif row['sentiment'] in ['enthusiasm', 'happiness', 'love', 'fun']:\n",
    "    #    return 'Positive'\n",
    "    #elif row['sentiment'] in ['neutral']:\n",
    "       # return 'Neutral'\n",
    "    else:\n",
    "        return 'Undefined'  # Handle any cases that don't fit the above categories\n",
    "\n",
    "# Apply the function along the rows of the DataFrame\n",
    "df['category'] = df.apply(categorize_sentiment, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kePuY1niyNR0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df.rename(columns={'sentiment': 'category'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "6kLSrApfMAnQ",
    "outputId": "ee38a83b-ac35-4cb1-8842-da3731f41bd6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVGYMWlHxfDE"
   },
   "source": [
    "# Balance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "id": "wy-fJSlHxfDE",
    "outputId": "33ebfb5e-d5c0-434a-de26-e2888775ba83",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.histplot(x=df['category'], kde=True)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pEfW7VldxfDF",
    "outputId": "030bb155-ee8e-4321-a859-df2cb129b8c3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "t53jBTygxfDF",
    "outputId": "99cb40b0-2a78-44a3-d018-27deb3563bed",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DpZK0fHbxfDF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_mapping = {'Neutral': 0, 'Concerned': 1}#, 'Concerned':2}, 'happiness': 2, 'sadness':3, 'love':4, 'surprise':5,'fun':6, 'relief':7, 'hate':8, 'empty':9,\n",
    "              #  'enthusiasm':10, 'boredom':11,'anger':12}\n",
    "df['category'] = df['category'].map(label_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_xgAar2jxfDF",
    "outputId": "2a2cf9a5-35ef-409e-f6b1-d4c6c8041ac1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "print(f'Train: {len(train_df)}; Test: {len(test_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "8dDUz6KLxfDG",
    "outputId": "e3cbb7e6-eb8d-4169-ffc3-feb3229cbe97",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_53ctV8QxfDG",
    "outputId": "082f92e1-70d9-4218-84a1-52cef70cecfd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = train_df['content'].values\n",
    "y_train = train_df['category'].values\n",
    "\n",
    "X = test_df['content'].values\n",
    "y = test_df['category'].values\n",
    "\n",
    "test_df, val_df = train_test_split(test_df, test_size=0.25)\n",
    "\n",
    "X_valid = val_df['content'].values\n",
    "y_valid = val_df['sentiment'].values\n",
    "\n",
    "X_test = test_df['content'].values\n",
    "y_test = test_df['sentiment'].values\n",
    "\n",
    "print(f'X_train: {len(X_train)}; X_test: {len(X_test)}; X_valid: {len(X_valid)}')\n",
    "print(f'y_train: {len(y_train)}; y_test: {len(y_test)}; y_valid: {len(y_valid)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GauA0nGyxfDG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "y_train = encoder.fit_transform(np.array(y_train).reshape(-1, 1)).toarray()\n",
    "y_test = encoder.fit_transform(np.array(y_test).reshape(-1, 1)).toarray()\n",
    "y_valid = encoder.fit_transform(np.array(y_valid).reshape(-1, 1)).toarray()\n",
    "labels = np.unique(encoder.inverse_transform(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wO26RxTIxfDG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = df['category'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lv2g_VsLxfDH",
    "outputId": "19de5418-90e6-45c5-f322-ce0b5db3093e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import XLNetForSequenceClassification\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import XLNetTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'xlnet-base-cased'\n",
    "tokenizer = XLNetTokenizer.from_pretrained(model_name)\n",
    "model = XLNetForSequenceClassification.from_pretrained(model_name, num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_B6MBsvxfDH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the categories to numerical labels\n",
    "train_df['category'] = label_encoder.fit_transform(train_df['category'])\n",
    "test_df['category'] = label_encoder.transform(test_df['category'])\n",
    "val_df['category'] = label_encoder.transform(val_df['category'])\n",
    "\n",
    "# Now the categories are numerical, you can convert them to tensors\n",
    "y_train = torch.tensor(train_df['category'].values)\n",
    "y_test = torch.tensor(test_df['category'].values)\n",
    "y_valid = torch.tensor(val_df['category'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hg4m1f-OxfDH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize and preprocess the text data\n",
    "def tokenize_text(text):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True, padding=True, truncation=True)\n",
    "    return tokens\n",
    "\n",
    "train_df['TokenizedText'] = train_df['content'].apply(tokenize_text)\n",
    "test_df['TokenizedText'] = test_df['content'].apply(tokenize_text)\n",
    "val_df['TokenizedText'] = val_df['content'].apply(tokenize_text)\n",
    "\n",
    "# Convert tokenized data to PyTorch tensors with padding\n",
    "X_train = pad_sequence([torch.tensor(seq) for seq in train_df['TokenizedText']], batch_first=True)\n",
    "y_train = torch.tensor(train_df['category'].tolist())\n",
    "X_test = pad_sequence([torch.tensor(seq) for seq in test_df['TokenizedText']], batch_first=True)\n",
    "y_test = torch.tensor(test_df['category'].tolist())\n",
    "X_valid = pad_sequence([torch.tensor(seq) for seq in val_df['TokenizedText']], batch_first=True)\n",
    "y_valid = torch.tensor(val_df['category'].tolist())\n",
    "\n",
    "# Define a DataLoader for batching data\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)\n",
    "val_dataset = TensorDataset(X_valid, y_valid)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Define the training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, labels=labels)  # Attention mask added by default\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H6YJ_tLjxfDI",
    "outputId": "84fc06b5-5946-4bdc-aaa3-16c7253c8e69",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, target_names=label_mapping.keys())\n",
    "    return cm, report\n",
    "\n",
    "for epoch in range(10):  # Run for 10 epochs\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{10}, Train Loss: {train_loss:.4f}\")\n",
    "# Evaluate on test and validation sets\n",
    "test_cm, test_report = evaluate(model, test_dataloader)\n",
    "print(\"Test Set Evaluation:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(test_cm)\n",
    "print(\"Classification Report:\")\n",
    "print(test_report)\n",
    "\n",
    "valid_cm, valid_report = evaluate(model, val_dataloader)\n",
    "print(\"Validation Set Evaluation:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(valid_cm)\n",
    "print(\"Classification Report:\")\n",
    "print(valid_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P8NBQBezxfDI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def categorize_sentiment(row):\n",
    "    if row['sentiment'] in ['empty', 'sadness', 'anger', 'worry', 'hate', 'boredom']:\n",
    "        return 'Concerned'\n",
    "    elif row['sentiment'] in ['neutral']:  # Note: 'empty' appears again; check if this is correct.\n",
    "        return 'Neutral'\n",
    "    elif row['sentiment'] in ['enthusiasm', 'happiness', 'love', 'fun','relief','surprise']:\n",
    "        return 'Positive'\n",
    "    #elif row['sentiment'] in ['neutral']:\n",
    "       # return 'Neutral'\n",
    "    else:\n",
    "        return 'Undefined'  # Handle any cases that don't fit the above categories\n",
    "\n",
    "# Apply the function along the rows of the DataFrame\n",
    "df['category'] = df.apply(categorize_sentiment, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "Xob5fhFzPy-I",
    "outputId": "1af9bd96-d3fe-4a2f-dd45-cbc7a4114e96"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "syjIpT7kHeUg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_mapping = {'Neutral': 0, 'Concerned': 1, 'Positive':2}# 'happiness': 2, 'sadness':3, 'love':4, 'surprise':5,'fun':6, 'relief':7, 'hate':8, 'empty':9,\n",
    "              #  'enthusiasm':10, 'boredom':11,'anger':12}\n",
    "df['category'] = df['category'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O2Lueg9iHha-",
    "outputId": "4ba0dbd4-4c0c-4de1-fc21-799c371c754e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "print(f'Train: {len(train_df)}; Test: {len(test_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T3zSDaAZHmqs",
    "outputId": "771ce28e-0939-45b5-864b-a1308328bb54",
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = test_df['content'].values\n",
    "y = test_df['category'].values\n",
    "\n",
    "test_df, val_df = train_test_split(test_df, test_size=0.25)\n",
    "\n",
    "X_valid = val_df['content'].values\n",
    "y_valid = val_df['sentiment'].values\n",
    "\n",
    "X_test = test_df['content'].values\n",
    "y_test = test_df['sentiment'].values\n",
    "\n",
    "print(f'X_train: {len(X_train)}; X_test: {len(X_test)}; X_valid: {len(X_valid)}')\n",
    "print(f'y_train: {len(y_train)}; y_test: {len(y_test)}; y_valid: {len(y_valid)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KlV0YVOYHpgw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "y_train = encoder.fit_transform(np.array(y_train).reshape(-1, 1)).toarray()\n",
    "y_test = encoder.fit_transform(np.array(y_test).reshape(-1, 1)).toarray()\n",
    "y_valid = encoder.fit_transform(np.array(y_valid).reshape(-1, 1)).toarray()\n",
    "labels = np.unique(encoder.inverse_transform(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0RtZPf8sHsTP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = df['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PURsRASqHurT",
    "outputId": "3d75ee98-f963-4150-c4e2-ae300a4a8736",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'xlnet-base-cased'\n",
    "tokenizer = XLNetTokenizer.from_pretrained(model_name)\n",
    "model = XLNetForSequenceClassification.from_pretrained(model_name, num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "edakaBMKHxFS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "is_train=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYvV8Ha5H0uI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the categories to numerical labels\n",
    "train_df['category'] = label_encoder.fit_transform(train_df['category'])\n",
    "test_df['category'] = label_encoder.transform(test_df['category'])\n",
    "val_df['category'] = label_encoder.transform(val_df['category'])\n",
    "\n",
    "# Now the categories are numerical, you can convert them to tensors\n",
    "y_train = torch.tensor(train_df['category'].values)\n",
    "y_test = torch.tensor(test_df['category'].values)\n",
    "y_valid = torch.tensor(val_df['category'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZRspBboH3Oa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize and preprocess the text data\n",
    "def tokenize_text(text):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True, padding=True, truncation=True)\n",
    "    return tokens\n",
    "\n",
    "train_df['TokenizedText'] = train_df['content'].apply(tokenize_text)\n",
    "test_df['TokenizedText'] = test_df['content'].apply(tokenize_text)\n",
    "val_df['TokenizedText'] = val_df['content'].apply(tokenize_text)\n",
    "\n",
    "# Convert tokenized data to PyTorch tensors with padding\n",
    "X_train = pad_sequence([torch.tensor(seq) for seq in train_df['TokenizedText']], batch_first=True)\n",
    "y_train = torch.tensor(train_df['category'].tolist())\n",
    "X_test = pad_sequence([torch.tensor(seq) for seq in test_df['TokenizedText']], batch_first=True)\n",
    "y_test = torch.tensor(test_df['category'].tolist())\n",
    "X_valid = pad_sequence([torch.tensor(seq) for seq in val_df['TokenizedText']], batch_first=True)\n",
    "y_valid = torch.tensor(val_df['category'].tolist())\n",
    "\n",
    "# Define a DataLoader for batching data\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)\n",
    "val_dataset = TensorDataset(X_valid, y_valid)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Define the training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, labels=labels)  # Attention mask added by default\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EmcTdc5KH6Ty",
    "outputId": "dbaa14ad-7680-4b5d-be85-8623b2a8f89e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, target_names=label_mapping.keys())\n",
    "    return cm, report\n",
    "\n",
    "for epoch in range(10):  # Run for 10 epochs\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{10}, Train Loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EmcTdc5KH6Ty",
    "outputId": "dbaa14ad-7680-4b5d-be85-8623b2a8f89e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_cm, valid_report = evaluate(model, val_dataloader)\n",
    "print(\"Validation Set Evaluation:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(valid_cm)\n",
    "print(\"Classification Report:\")\n",
    "print(valid_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate on test and validation sets\n",
    "test_cm, test_report = evaluate(model, test_dataloader)\n",
    "print(\"Test Set Evaluation:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(test_cm)\n",
    "print(\"Classification Report:\")\n",
    "print(test_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 32 Batch Size and 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'xlnet-base-cased'\n",
    "model = XLNetForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
    "is_train=True\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the categories to numerical labels\n",
    "train_df['category'] = label_encoder.fit_transform(train_df['category'])\n",
    "test_df['category'] = label_encoder.transform(test_df['category'])\n",
    "val_df['category'] = label_encoder.transform(val_df['category'])\n",
    "\n",
    "# Now the categories are numerical, you can convert them to tensors\n",
    "y_train = torch.tensor(train_df['category'].values)\n",
    "y_test = torch.tensor(test_df['category'].values)\n",
    "y_valid = torch.tensor(val_df['category'].values)\n",
    "\n",
    "# Tokenize and preprocess the text data\n",
    "def tokenize_text(text):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True, padding=True, truncation=True)\n",
    "    return tokens\n",
    "\n",
    "train_df['TokenizedText'] = train_df['content'].apply(tokenize_text)\n",
    "test_df['TokenizedText'] = test_df['content'].apply(tokenize_text)\n",
    "val_df['TokenizedText'] = val_df['content'].apply(tokenize_text)\n",
    "\n",
    "# Convert tokenized data to PyTorch tensors with padding\n",
    "X_train = pad_sequence([torch.tensor(seq) for seq in train_df['TokenizedText']], batch_first=True)\n",
    "y_train = torch.tensor(train_df['category'].tolist())\n",
    "X_test = pad_sequence([torch.tensor(seq) for seq in test_df['TokenizedText']], batch_first=True)\n",
    "y_test = torch.tensor(test_df['category'].tolist())\n",
    "X_valid = pad_sequence([torch.tensor(seq) for seq in val_df['TokenizedText']], batch_first=True)\n",
    "y_valid = torch.tensor(val_df['category'].tolist())\n",
    "\n",
    "# Define a DataLoader for batching data\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)\n",
    "val_dataset = TensorDataset(X_valid, y_valid)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Define the training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, labels=labels)  # Attention mask added by default\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, target_names=label_mapping.keys())\n",
    "    return cm, report\n",
    "\n",
    "for epoch in range(10):  # Run for 10 epochs\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{10}, Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "valid_cm, valid_report = evaluate(model, val_dataloader)\n",
    "print(\"Validation Set Evaluation:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(valid_cm)\n",
    "print(\"Classification Report:\")\n",
    "print(valid_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 32 Batch Size and 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'xlnet-base-cased'\n",
    "model = XLNetForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
    "is_train=True\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the categories to numerical labels\n",
    "train_df['category'] = label_encoder.fit_transform(train_df['category'])\n",
    "test_df['category'] = label_encoder.transform(test_df['category'])\n",
    "val_df['category'] = label_encoder.transform(val_df['category'])\n",
    "\n",
    "# Now the categories are numerical, you can convert them to tensors\n",
    "y_train = torch.tensor(train_df['category'].values)\n",
    "y_test = torch.tensor(test_df['category'].values)\n",
    "y_valid = torch.tensor(val_df['category'].values)\n",
    "\n",
    "# Tokenize and preprocess the text data\n",
    "def tokenize_text(text):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True, padding=True, truncation=True)\n",
    "    return tokens\n",
    "\n",
    "train_df['TokenizedText'] = train_df['content'].apply(tokenize_text)\n",
    "test_df['TokenizedText'] = test_df['content'].apply(tokenize_text)\n",
    "val_df['TokenizedText'] = val_df['content'].apply(tokenize_text)\n",
    "\n",
    "# Convert tokenized data to PyTorch tensors with padding\n",
    "X_train = pad_sequence([torch.tensor(seq) for seq in train_df['TokenizedText']], batch_first=True)\n",
    "y_train = torch.tensor(train_df['category'].tolist())\n",
    "X_test = pad_sequence([torch.tensor(seq) for seq in test_df['TokenizedText']], batch_first=True)\n",
    "y_test = torch.tensor(test_df['category'].tolist())\n",
    "X_valid = pad_sequence([torch.tensor(seq) for seq in val_df['TokenizedText']], batch_first=True)\n",
    "y_valid = torch.tensor(val_df['category'].tolist())\n",
    "\n",
    "# Define a DataLoader for batching data\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)\n",
    "val_dataset = TensorDataset(X_valid, y_valid)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Define the training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, labels=labels)  # Attention mask added by default\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, target_names=label_mapping.keys())\n",
    "    return cm, report\n",
    "\n",
    "for epoch in range(10):  # Run for 10 epochs\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{10}, Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "valid_cm, valid_report = evaluate(model, val_dataloader)\n",
    "print(\"Validation Set Evaluation:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(valid_cm)\n",
    "print(\"Classification Report:\")\n",
    "print(valid_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 32 Batch Size and 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'xlnet-base-cased'\n",
    "model = XLNetForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
    "is_train=True\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the categories to numerical labels\n",
    "train_df['category'] = label_encoder.fit_transform(train_df['category'])\n",
    "test_df['category'] = label_encoder.transform(test_df['category'])\n",
    "val_df['category'] = label_encoder.transform(val_df['category'])\n",
    "\n",
    "# Now the categories are numerical, you can convert them to tensors\n",
    "y_train = torch.tensor(train_df['category'].values)\n",
    "y_test = torch.tensor(test_df['category'].values)\n",
    "y_valid = torch.tensor(val_df['category'].values)\n",
    "\n",
    "# Tokenize and preprocess the text data\n",
    "def tokenize_text(text):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True, padding=True, truncation=True)\n",
    "    return tokens\n",
    "\n",
    "train_df['TokenizedText'] = train_df['content'].apply(tokenize_text)\n",
    "test_df['TokenizedText'] = test_df['content'].apply(tokenize_text)\n",
    "val_df['TokenizedText'] = val_df['content'].apply(tokenize_text)\n",
    "\n",
    "# Convert tokenized data to PyTorch tensors with padding\n",
    "X_train = pad_sequence([torch.tensor(seq) for seq in train_df['TokenizedText']], batch_first=True)\n",
    "y_train = torch.tensor(train_df['category'].tolist())\n",
    "X_test = pad_sequence([torch.tensor(seq) for seq in test_df['TokenizedText']], batch_first=True)\n",
    "y_test = torch.tensor(test_df['category'].tolist())\n",
    "X_valid = pad_sequence([torch.tensor(seq) for seq in val_df['TokenizedText']], batch_first=True)\n",
    "y_valid = torch.tensor(val_df['category'].tolist())\n",
    "\n",
    "# Define a DataLoader for batching data\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)\n",
    "val_dataset = TensorDataset(X_valid, y_valid)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Define the training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-8)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, labels=labels)  # Attention mask added by default\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, target_names=label_mapping.keys())\n",
    "    return cm, report\n",
    "\n",
    "for epoch in range(10):  # Run for 10 epochs\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{10}, Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "valid_cm, valid_report = evaluate(model, val_dataloader)\n",
    "print(\"Validation Set Evaluation:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(valid_cm)\n",
    "print(\"Classification Report:\")\n",
    "print(valid_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 64 Batch Size and 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'xlnet-base-cased'\n",
    "tokenizer = XLNetTokenizer.from_pretrained(model_name)\n",
    "model = XLNetForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
    "is_train=True\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the categories to numerical labels\n",
    "train_df['category'] = label_encoder.fit_transform(train_df['category'])\n",
    "test_df['category'] = label_encoder.transform(test_df['category'])\n",
    "val_df['category'] = label_encoder.transform(val_df['category'])\n",
    "\n",
    "# Now the categories are numerical, you can convert them to tensors\n",
    "y_train = torch.tensor(train_df['category'].values)\n",
    "y_test = torch.tensor(test_df['category'].values)\n",
    "y_valid = torch.tensor(val_df['category'].values)\n",
    "\n",
    "# Tokenize and preprocess the text data\n",
    "def tokenize_text(text):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True, padding=True, truncation=True)\n",
    "    return tokens\n",
    "\n",
    "train_df['TokenizedText'] = train_df['content'].apply(tokenize_text)\n",
    "test_df['TokenizedText'] = test_df['content'].apply(tokenize_text)\n",
    "val_df['TokenizedText'] = val_df['content'].apply(tokenize_text)\n",
    "\n",
    "# Convert tokenized data to PyTorch tensors with padding\n",
    "X_train = pad_sequence([torch.tensor(seq) for seq in train_df['TokenizedText']], batch_first=True)\n",
    "y_train = torch.tensor(train_df['category'].tolist())\n",
    "X_test = pad_sequence([torch.tensor(seq) for seq in test_df['TokenizedText']], batch_first=True)\n",
    "y_test = torch.tensor(test_df['category'].tolist())\n",
    "X_valid = pad_sequence([torch.tensor(seq) for seq in val_df['TokenizedText']], batch_first=True)\n",
    "y_valid = torch.tensor(val_df['category'].tolist())\n",
    "\n",
    "# Define a DataLoader for batching data\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)\n",
    "val_dataset = TensorDataset(X_valid, y_valid)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Define the training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, labels=labels)  # Attention mask added by default\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, target_names=label_mapping.keys())\n",
    "    return cm, report\n",
    "\n",
    "for epoch in range(10):  # Run for 10 epochs\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{10}, Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "valid_cm, valid_report = evaluate(model, val_dataloader)\n",
    "print(\"Validation Set Evaluation:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(valid_cm)\n",
    "print(\"Classification Report:\")\n",
    "print(valid_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 64 Batch Size and 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'xlnet-base-cased'\n",
    "model = XLNetForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
    "is_train=True\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the categories to numerical labels\n",
    "train_df['category'] = label_encoder.fit_transform(train_df['category'])\n",
    "test_df['category'] = label_encoder.transform(test_df['category'])\n",
    "val_df['category'] = label_encoder.transform(val_df['category'])\n",
    "\n",
    "# Now the categories are numerical, you can convert them to tensors\n",
    "y_train = torch.tensor(train_df['category'].values)\n",
    "y_test = torch.tensor(test_df['category'].values)\n",
    "y_valid = torch.tensor(val_df['category'].values)\n",
    "\n",
    "# Tokenize and preprocess the text data\n",
    "def tokenize_text(text):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True, padding=True, truncation=True)\n",
    "    return tokens\n",
    "\n",
    "train_df['TokenizedText'] = train_df['content'].apply(tokenize_text)\n",
    "test_df['TokenizedText'] = test_df['content'].apply(tokenize_text)\n",
    "val_df['TokenizedText'] = val_df['content'].apply(tokenize_text)\n",
    "\n",
    "# Convert tokenized data to PyTorch tensors with padding\n",
    "X_train = pad_sequence([torch.tensor(seq) for seq in train_df['TokenizedText']], batch_first=True)\n",
    "y_train = torch.tensor(train_df['category'].tolist())\n",
    "X_test = pad_sequence([torch.tensor(seq) for seq in test_df['TokenizedText']], batch_first=True)\n",
    "y_test = torch.tensor(test_df['category'].tolist())\n",
    "X_valid = pad_sequence([torch.tensor(seq) for seq in val_df['TokenizedText']], batch_first=True)\n",
    "y_valid = torch.tensor(val_df['category'].tolist())\n",
    "\n",
    "# Define a DataLoader for batching data\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)\n",
    "val_dataset = TensorDataset(X_valid, y_valid)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Define the training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, labels=labels)  # Attention mask added by default\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, target_names=label_mapping.keys())\n",
    "    return cm, report\n",
    "\n",
    "for epoch in range(10):  # Run for 10 epochs\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{10}, Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "valid_cm, valid_report = evaluate(model, val_dataloader)\n",
    "print(\"Validation Set Evaluation:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(valid_cm)\n",
    "print(\"Classification Report:\")\n",
    "print(valid_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 64 Batch Size and 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'xlnet-base-cased'\n",
    "tokenizer = XLNetTokenizer.from_pretrained(model_name)\n",
    "model = XLNetForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
    "is_train=True\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the categories to numerical labels\n",
    "train_df['category'] = label_encoder.fit_transform(train_df['category'])\n",
    "test_df['category'] = label_encoder.transform(test_df['category'])\n",
    "val_df['category'] = label_encoder.transform(val_df['category'])\n",
    "\n",
    "# Now the categories are numerical, you can convert them to tensors\n",
    "y_train = torch.tensor(train_df['category'].values)\n",
    "y_test = torch.tensor(test_df['category'].values)\n",
    "y_valid = torch.tensor(val_df['category'].values)\n",
    "\n",
    "# Tokenize and preprocess the text data\n",
    "def tokenize_text(text):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True, padding=True, truncation=True)\n",
    "    return tokens\n",
    "\n",
    "train_df['TokenizedText'] = train_df['content'].apply(tokenize_text)\n",
    "test_df['TokenizedText'] = test_df['content'].apply(tokenize_text)\n",
    "val_df['TokenizedText'] = val_df['content'].apply(tokenize_text)\n",
    "\n",
    "# Convert tokenized data to PyTorch tensors with padding\n",
    "X_train = pad_sequence([torch.tensor(seq) for seq in train_df['TokenizedText']], batch_first=True)\n",
    "y_train = torch.tensor(train_df['category'].tolist())\n",
    "X_test = pad_sequence([torch.tensor(seq) for seq in test_df['TokenizedText']], batch_first=True)\n",
    "y_test = torch.tensor(test_df['category'].tolist())\n",
    "X_valid = pad_sequence([torch.tensor(seq) for seq in val_df['TokenizedText']], batch_first=True)\n",
    "y_valid = torch.tensor(val_df['category'].tolist())\n",
    "\n",
    "# Define a DataLoader for batching data\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)\n",
    "val_dataset = TensorDataset(X_valid, y_valid)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Define the training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-8)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, labels=labels)  # Attention mask added by default\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, target_names=label_mapping.keys())\n",
    "    return cm, report\n",
    "\n",
    "for epoch in range(10):  # Run for 10 epochs\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{10}, Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "valid_cm, valid_report = evaluate(model, val_dataloader)\n",
    "print(\"Validation Set Evaluation:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(valid_cm)\n",
    "print(\"Classification Report:\")\n",
    "print(valid_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 128 Batch Size and 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'xlnet-base-cased'\n",
    "model = XLNetForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
    "is_train=True\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the categories to numerical labels\n",
    "train_df['category'] = label_encoder.fit_transform(train_df['category'])\n",
    "test_df['category'] = label_encoder.transform(test_df['category'])\n",
    "val_df['category'] = label_encoder.transform(val_df['category'])\n",
    "\n",
    "# Now the categories are numerical, you can convert them to tensors\n",
    "y_train = torch.tensor(train_df['category'].values)\n",
    "y_test = torch.tensor(test_df['category'].values)\n",
    "y_valid = torch.tensor(val_df['category'].values)\n",
    "\n",
    "# Tokenize and preprocess the text data\n",
    "def tokenize_text(text):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True, padding=True, truncation=True)\n",
    "    return tokens\n",
    "\n",
    "train_df['TokenizedText'] = train_df['content'].apply(tokenize_text)\n",
    "test_df['TokenizedText'] = test_df['content'].apply(tokenize_text)\n",
    "val_df['TokenizedText'] = val_df['content'].apply(tokenize_text)\n",
    "\n",
    "# Convert tokenized data to PyTorch tensors with padding\n",
    "X_train = pad_sequence([torch.tensor(seq) for seq in train_df['TokenizedText']], batch_first=True)\n",
    "y_train = torch.tensor(train_df['category'].tolist())\n",
    "X_test = pad_sequence([torch.tensor(seq) for seq in test_df['TokenizedText']], batch_first=True)\n",
    "y_test = torch.tensor(test_df['category'].tolist())\n",
    "X_valid = pad_sequence([torch.tensor(seq) for seq in val_df['TokenizedText']], batch_first=True)\n",
    "y_valid = torch.tensor(val_df['category'].tolist())\n",
    "\n",
    "# Define a DataLoader for batching data\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128)\n",
    "val_dataset = TensorDataset(X_valid, y_valid)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128)\n",
    "\n",
    "# Define the training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, labels=labels)  # Attention mask added by default\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, target_names=label_mapping.keys())\n",
    "    return cm, report\n",
    "\n",
    "for epoch in range(10):  # Run for 10 epochs\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{10}, Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "valid_cm, valid_report = evaluate(model, val_dataloader)\n",
    "print(\"Validation Set Evaluation:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(valid_cm)\n",
    "print(\"Classification Report:\")\n",
    "print(valid_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 128 Batch Size and 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'xlnet-base-cased'\n",
    "model = XLNetForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
    "is_train=True\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the categories to numerical labels\n",
    "train_df['category'] = label_encoder.fit_transform(train_df['category'])\n",
    "test_df['category'] = label_encoder.transform(test_df['category'])\n",
    "val_df['category'] = label_encoder.transform(val_df['category'])\n",
    "\n",
    "# Now the categories are numerical, you can convert them to tensors\n",
    "y_train = torch.tensor(train_df['category'].values)\n",
    "y_test = torch.tensor(test_df['category'].values)\n",
    "y_valid = torch.tensor(val_df['category'].values)\n",
    "\n",
    "# Tokenize and preprocess the text data\n",
    "def tokenize_text(text):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True, padding=True, truncation=True)\n",
    "    return tokens\n",
    "\n",
    "train_df['TokenizedText'] = train_df['content'].apply(tokenize_text)\n",
    "test_df['TokenizedText'] = test_df['content'].apply(tokenize_text)\n",
    "val_df['TokenizedText'] = val_df['content'].apply(tokenize_text)\n",
    "\n",
    "# Convert tokenized data to PyTorch tensors with padding\n",
    "X_train = pad_sequence([torch.tensor(seq) for seq in train_df['TokenizedText']], batch_first=True)\n",
    "y_train = torch.tensor(train_df['category'].tolist())\n",
    "X_test = pad_sequence([torch.tensor(seq) for seq in test_df['TokenizedText']], batch_first=True)\n",
    "y_test = torch.tensor(test_df['category'].tolist())\n",
    "X_valid = pad_sequence([torch.tensor(seq) for seq in val_df['TokenizedText']], batch_first=True)\n",
    "y_valid = torch.tensor(val_df['category'].tolist())\n",
    "\n",
    "# Define a DataLoader for batching data\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128)\n",
    "val_dataset = TensorDataset(X_valid, y_valid)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128)\n",
    "\n",
    "# Define the training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, labels=labels)  # Attention mask added by default\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, target_names=label_mapping.keys())\n",
    "    return cm, report\n",
    "\n",
    "for epoch in range(10):  # Run for 10 epochs\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{10}, Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "valid_cm, valid_report = evaluate(model, val_dataloader)\n",
    "print(\"Validation Set Evaluation:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(valid_cm)\n",
    "print(\"Classification Report:\")\n",
    "print(valid_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 128 Batch Size and 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'xlnet-base-cased'\n",
    "model = XLNetForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
    "is_train=True\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the categories to numerical labels\n",
    "train_df['category'] = label_encoder.fit_transform(train_df['category'])\n",
    "test_df['category'] = label_encoder.transform(test_df['category'])\n",
    "val_df['category'] = label_encoder.transform(val_df['category'])\n",
    "\n",
    "# Now the categories are numerical, you can convert them to tensors\n",
    "y_train = torch.tensor(train_df['category'].values)\n",
    "y_test = torch.tensor(test_df['category'].values)\n",
    "y_valid = torch.tensor(val_df['category'].values)\n",
    "\n",
    "# Tokenize and preprocess the text data\n",
    "def tokenize_text(text):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True, padding=True, truncation=True)\n",
    "    return tokens\n",
    "\n",
    "train_df['TokenizedText'] = train_df['content'].apply(tokenize_text)\n",
    "test_df['TokenizedText'] = test_df['content'].apply(tokenize_text)\n",
    "val_df['TokenizedText'] = val_df['content'].apply(tokenize_text)\n",
    "\n",
    "# Convert tokenized data to PyTorch tensors with padding\n",
    "X_train = pad_sequence([torch.tensor(seq) for seq in train_df['TokenizedText']], batch_first=True)\n",
    "y_train = torch.tensor(train_df['category'].tolist())\n",
    "X_test = pad_sequence([torch.tensor(seq) for seq in test_df['TokenizedText']], batch_first=True)\n",
    "y_test = torch.tensor(test_df['category'].tolist())\n",
    "X_valid = pad_sequence([torch.tensor(seq) for seq in val_df['TokenizedText']], batch_first=True)\n",
    "y_valid = torch.tensor(val_df['category'].tolist())\n",
    "\n",
    "# Define a DataLoader for batching data\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128)\n",
    "val_dataset = TensorDataset(X_valid, y_valid)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128)\n",
    "\n",
    "# Define the training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, labels=labels)  # Attention mask added by default\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, target_names=label_mapping.keys())\n",
    "    return cm, report\n",
    "\n",
    "for epoch in range(10):  # Run for 10 epochs\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{10}, Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "valid_cm, valid_report = evaluate(model, val_dataloader)\n",
    "print(\"Validation Set Evaluation:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(valid_cm)\n",
    "print(\"Classification Report:\")\n",
    "print(valid_report)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PyTorch 2.0.1",
   "language": "python",
   "name": "pytorch-2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
